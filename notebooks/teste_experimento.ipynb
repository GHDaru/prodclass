{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste da classe de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Ajuste o caminho conforme a necessidade\n",
    "# Passo 1: Importação de Bibliotecas Necessárias\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from models.tabela_experimentos import TabelaExperimentos\n",
    "from models.tabela_modelos import TabelaModelos\n",
    "from models.tabela_execucoes import TabelaExecucoes\n",
    "from experiment_manager.experiment_manager import ExperimentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 2: Criação de um Conjunto de Dados Sintético\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 4: Configuração do Classificador e Parâmetros para Teste\n",
    "classifier = LogisticRegression(solver='liblinear')\n",
    "parameter_dict = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento já existe. Retornando o ID existente.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parametros</th>\n",
       "      <th>Rodada</th>\n",
       "      <th>Acuracia</th>\n",
       "      <th>F1ScoreMacro</th>\n",
       "      <th>TempoProcessamento</th>\n",
       "      <th>DataExecucao</th>\n",
       "      <th>Estatisticas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Parametros, Rodada, Acuracia, F1ScoreMacro, TempoProcessamento, DataExecucao, Estatisticas]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instanciando a ExperimentManager\n",
    "db_path = '../dbs/meu_banco_de_dados.db'  # Ajuste o caminho para o seu banco de dados\n",
    "project_name = 'TesteClassificador'\n",
    "experiment_manager = ExperimentManager(project_name, db_path)\n",
    "# Passo 5: Execução do Experimento\n",
    "experiment_name = 'Experimento_LogisticRegression2'\n",
    "experiment_manager.run(experiment_name, classifier, X_train, y_train, parameter_dict, folds=5, random_state=42)\n",
    "# Obter e Exibir os Resultados do Experimento\n",
    "experiment_manager.get_experiment_results(experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Parametros, Rodada, Acuracia, F1ScoreMacro, TempoProcessamento, DataExecucao, Estatisticas]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.tabela_experimentos import TabelaExperimentos\n",
    "from src.models.tabela_modelos import TabelaModelos\n",
    "from src.models.tabela_execucoes import TabelaExecucoes\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"Gestor de experimentos para avaliação de modelos de machine learning.\"\"\"\n",
    "\n",
    "    def __init__(self, project_name, db_path):\n",
    "        \"\"\"Inicializa o gestor de experimentos e as tabelas associadas.\n",
    "\n",
    "        Args:\n",
    "            project_name (str): Nome do projeto.\n",
    "            db_path (str): Caminho para o arquivo do banco de dados.\n",
    "        \"\"\"\n",
    "        self.project_name = project_name\n",
    "        self.db_path = db_path\n",
    "        # Inicializa as classes de tabelas\n",
    "        self.tabela_experimentos = TabelaExperimentos(self.db_path)\n",
    "        self.tabela_modelos = TabelaModelos(self.db_path)\n",
    "        self.tabela_execucoes = TabelaExecucoes(self.db_path)\n",
    "\n",
    "    def add_experiment(self, experiment_name):\n",
    "        \"\"\"Adiciona um novo experimento ao banco de dados, se não existir.\n",
    "\n",
    "        Args:\n",
    "            experiment_name (str): Nome do experimento.\n",
    "\n",
    "        Returns:\n",
    "            int: O ID do experimento adicionado.\n",
    "        \"\"\"\n",
    "        return self.tabela_experimentos.add(experiment_name, self.project_name)\n",
    "\n",
    "    def add_model_variation(self, experiment_id, parameters, seed):\n",
    "        \"\"\"Adiciona uma variação de modelo ao banco de dados se não existir.\n",
    "\n",
    "        Args:\n",
    "            experiment_id (int): ID do experimento ao qual o modelo pertence.\n",
    "            parameters (dict): Parâmetros do modelo.\n",
    "            seed (int): Semente utilizada para a inicialização do modelo.\n",
    "\n",
    "        Returns:\n",
    "            int: O ID da variação do modelo adicionada.\n",
    "        \"\"\"\n",
    "        return self.tabela_modelos.add(experiment_id, parameters, seed)\n",
    "\n",
    "    def record_execution(self, model_id, round, accuracy, f1_score_macro, processing_time):\n",
    "        \"\"\"Registra os resultados e o tempo de processamento de uma execução do modelo no banco de dados.\n",
    "\n",
    "        Args:\n",
    "            model_id (int): ID do modelo que foi executado.\n",
    "            round (int): Número da rodada de validação cruzada.\n",
    "            accuracy (float): Acurácia alcançada na rodada.\n",
    "            f1_score_macro (float): F1-Score Macro alcançado na rodada.\n",
    "            processing_time (float): Tempo de processamento da execução em segundos.\n",
    "        \"\"\"\n",
    "        estatisticas = {\"accuracy\": accuracy, \"f1_score_macro\": f1_score_macro}\n",
    "        self.tabela_execucoes.add(model_id, round, accuracy, f1_score_macro, processing_time, estatisticas)\n",
    "\n",
    "\n",
    "    def generate_parameter_combinations(self, parameter_dict):\n",
    "        \"\"\"Gera todas as combinações possíveis dos parâmetros fornecidos.\n",
    "\n",
    "        Args:\n",
    "            parameter_dict (dict): Dicionário contendo os parâmetros e suas variações a serem testadas.\n",
    "\n",
    "        Returns:\n",
    "            list of dict: Lista contendo dicionários de cada combinação de parâmetros.\n",
    "        \"\"\"\n",
    "        return [dict(zip(parameter_dict, v)) for v in itertools.product(*parameter_dict.values())]\n",
    "\n",
    "    def get_experiment_results(self, experiment_name):\n",
    "        \"\"\"Recupera os resultados de um experimento específico.\n",
    "\n",
    "        Args:\n",
    "            experiment_name (str): Nome do experimento cujos resultados devem ser recuperados.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame contendo os resultados do experimento.\n",
    "        \"\"\"\n",
    "        # Correção: Use a consulta correta para recuperar o ID do experimento\n",
    "        consulta_experimento = f\"SELECT ID FROM Experimentos WHERE Nome = '{experiment_name}' AND Projeto = '{self.project_name}'\"\n",
    "        experiment_id_df = self.tabela_experimentos.query_sql(consulta_experimento)\n",
    "\n",
    "        if not experiment_id_df.empty:\n",
    "            experiment_id = experiment_id_df.iloc[0]['ID']\n",
    "            \n",
    "            # Constrói a consulta SQL para recuperar resultados associados ao experimento\n",
    "            consulta_resultados = f'''\n",
    "                SELECT m.Parametros, e.Rodada, e.Acuracia, e.F1ScoreMacro, e.TempoProcessamento, e.DataExecucao, e.Estatisticas\n",
    "                FROM Execucoes e\n",
    "                JOIN Modelos m ON e.ID_Modelo = m.ID\n",
    "                WHERE m.ID_Experimento = {experiment_id}\n",
    "            '''\n",
    "            return self.tabela_execucoes.query_sql(consulta_resultados)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def run(self, experiment_name, classifier, X, y, parameter_dict, folds=10, random_state=100):\n",
    "        \"\"\"Executa um experimento com todas as combinações de parâmetros especificadas.\n",
    "\n",
    "        Args:\n",
    "            experiment_name (str): Nome do experimento.\n",
    "            classifier (any): O classificador a ser usado no experimento.\n",
    "            X (array-like): Dados de entrada para o modelo.\n",
    "            y (array-like): Rótulos de saída para o modelo.\n",
    "            parameter_dict (dict): Dicionário contendo os parâmetros e suas variações a serem testadas.\n",
    "            folds (int): Número de dobras para a validação cruzada K-Fold.\n",
    "            random_state (int, optional): Semente para a reprodutibilidade dos resultados.\n",
    "        \"\"\"\n",
    "        experiment_id = self.add_experiment(experiment_name)\n",
    "        parameter_combinations = self.generate_parameter_combinations(parameter_dict)\n",
    "\n",
    "        for params in parameter_combinations:\n",
    "            model_id = self.add_model_variation(experiment_id, params, random_state)\n",
    "            # Chama evaluate_model para cada combinação de parâmetros\n",
    "            self.evaluate_model(model_id, classifier, X, y, params, folds, random_state)\n",
    "            \n",
    "    def evaluate_model(self, model_id, classifier, X, y, parameter_variation, folds=5, random_state=None):\n",
    "        \"\"\"Avalia o modelo com um conjunto específico de parâmetros usando KFold para validação cruzada.\n",
    "        ...\n",
    "        \"\"\"\n",
    "        # Configura o classificador com os parâmetros fornecidos\n",
    "        classifier.set_params(**parameter_variation)\n",
    "\n",
    "        # Configura o KFold\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "        for fold_idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            start_time = time.time()  # Inicia o cronômetro\n",
    "\n",
    "            # Treinamento e teste do modelo\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "\n",
    "            end_time = time.time()  # Finaliza o cronômetro\n",
    "            tempo_processamento = end_time - start_time\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "            estatisticas = {\n",
    "                \"quantidade_registros_treino\": len(train_index),\n",
    "                \"quantidade_registros_teste\": len(test_index)\n",
    "            }\n",
    "\n",
    "            # Registra os resultados da execução\n",
    "            self.tabela_execucoes.add(model_id, fold_idx, accuracy, f1_score_macro, tempo_processamento, estatisticas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
